# -*- coding: utf-8 -*-
"""student-performance-elevator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140975WCoovlnd6grmHEuZr7pgz76az7d
"""

!pip install groq --upgrade
!pip install openai --upgrade

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score
from groq import Groq
import openai
import gdown
from google.colab import userdata
import joblib
import os

"""# Load dataset"""

df = pd.read_csv("/content/student-performance-predictor.csv")

display(df)

"""# Correlation Matrix"""

df['absences'] = df['absences'].apply(lambda x: min(x, 30))

# Select only numeric features for correlation analysis
numeric_features = df.select_dtypes(include=np.number).columns
df_numeric = df[numeric_features]

# Compute correlation matrix using only numeric features
correlation_matrix = df_numeric.corr()

# Plot heatmap for correlation
plt.figure(figsize=(20, 12))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Matrix")
plt.show()

# Sort correlation with final_grade
correlation_matrix['final_grade'].sort_values(ascending=False)

"""# Preprocessing"""

def assign_grade(final_grade):
    if 0 <= final_grade <= 5:
        return 'D'
    elif 6 <= final_grade <= 10:
        return 'C'
    elif 11 <= final_grade <= 15:
        return 'B'
    elif 16 <= final_grade <= 20:
        return 'A'
    else:
        return 'Invalid'

# Apply the function to create the new column
df['grade'] = df['final_grade'].apply(assign_grade)

display(df)

"""### Check and Fill missing values"""

print("\nMissing values before filling:")
print(df.isnull().sum())

"""### Remove duplicates"""

print("\nDataset shape before removing duplicates:", df.shape)
df.drop_duplicates(inplace=True)
print("Dataset shape after removing duplicates:", df.shape)

"""### Check for Outliers"""

numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
print("\nChecking for outliers using boxplots...")
for col in numeric_columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot for {col}")
    plt.show()

outlier_counts = {}
outlier_rows = set()

for col in numeric_columns:
    Q1, Q3 = df[col].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
    outlier_counts[col] = outlier_mask.sum()
    outlier_rows.update(df[outlier_mask].index)

print("\nOutlier Counts per Column (including columns with 0 outliers):")
for col in numeric_columns:
    print(f"{col}: {outlier_counts.get(col, 0)} outliers")

print(f"\nTotal rows containing at least one outlier: {len(outlier_rows)}")

# df_clean = df.drop(index=outlier_rows)

df.drop(columns=['student_id'], inplace=True)

"""# Dropping irrelevant features from dataset

"""

columns_to_drop = [
    'parent_status',
    'romantic_relationship',
    'family_size',
    'father_education',
    'mother_job',
    'father_job',
    'guardian',
    'mother_education',
    'school'

]

# Drop the columns
df_cleaned = df.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
df_cleaned.head()

# Separate features and target
X = df_cleaned.drop(columns=['final_grade'])
y = df_cleaned['final_grade']

# Encode categorical features
categorical_cols = X.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Standardize numerical features
scaler = StandardScaler()
numerical_cols = X.select_dtypes(include=['int64']).columns
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Creates an instance of the Linear Regression model"""

lr = LinearRegression()

"""# Creates an instance of the Random Forest model"""

rf = RandomForestRegressor(n_estimators=100, random_state=42)

"""# Creates an instance of the Gradient Boosting model"""

gb = GradientBoostingRegressor(n_estimators=100, random_state=42)

"""# Creates an instance of the Support Vector Machine model"""

svm = SVR()

"""# Creates an instance of the K-Nearest Neighbors model"""

knn = KNeighborsRegressor(n_neighbors=5)

"""# Creates an instance of the Extra Tree Regressor model"""

etr = ExtraTreesRegressor(n_estimators=100, random_state=42)

"""# Training and Evaluating different models"""

def evaluate_model(name, model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    accuracy = (1 - (mse / np.var(y_test))) * 100
    mae = mean_absolute_error(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred)
    return {
        "Model": name,
        "MAE": mae,
        "RMSE": rmse,
        "R2 Score": r2,
        "Accuracy (%)": accuracy,
        "Explained Variance": evs
    }

# Collect all model results
results = [
    evaluate_model("Linear Regression", lr),
    evaluate_model("Random Forest", rf),
    evaluate_model("Gradient Boosting", gb),
    evaluate_model("Support Vector Machine", svm),
    evaluate_model("K-Nearest Neighbors", knn),
    evaluate_model("Extra Trees Regressor", etr)
]

# Display comparison
df = pd.DataFrame(results)
df_sorted = df.sort_values(by="R2 Score", ascending=False)

print("\nModel Comparison (sorted by R2 Score):")
print(df_sorted.to_string(index=False))

best_model = df_sorted.iloc[0]["Model"]
print(f"\nBest Model Based on R2 Score: {best_model}")

"""# Saving all trained models"""

# Save all trained models
joblib.dump(lr, "linear_regression_model.pth")
joblib.dump(rf, "random_forest_model.pth")
joblib.dump(gb, "gradient_boosting_model.pth")
joblib.dump(svm, "svm_model.pth")
joblib.dump(knn, "knn_model.pth")
joblib.dump(etr, "extra_trees_model.pth")

print("All models saved successfully:")
print("- linear_regression_model.pth")
print("- random_forest_model.pth")
print("- gradient_boosting_model.pth")
print("- svm_model.pth")
print("- knn_model.pth")
print("- extra_trees_model.pth")

"""# Providing suggestions"""

LIMIT = 10  # Change this to your desired limit
os.environ["GROQ_API_KEY"] = "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"
columns_to_drop = [
    'grade_1',
    'grade_2',
    'final_grade'
]

# Drop the columns
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

client = openai.OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=os.environ.get("GROQ_API_KEY")
)

suggestions = []

# Get the column names from df_cleaned
cleaned_columns = df_cleaned.columns.tolist()

# Loop through each row (with limit)
for i, (index, row) in enumerate(df_cleaned.iterrows()):
    if i >= LIMIT:
        break

    # Create a dictionary to hold the student's data
    student_data = {col: row[col] for col in cleaned_columns}

    # Construct the prompt
    prompt = f"""
You are a student success counselor. Given the following data about a student, provide personalized advice and support to help them improve their academic performance and overall well-being.

Here's the student's information:

"""
    for key, value in student_data.items():
        prompt += f"{key}: {value}\n"

    prompt += """
Based on this information, provide a single, concise suggestion regarding their grade.
If the grade is not 'A', suggest how to improve it to an 'A'.
If the grade is 'A', suggest how to maintain that level of performance.
Examples:
- "You can upgrade your grade from B to A."
- "You can make your grade consistent at A."
"""

    messages = [{"role": "user", "content": prompt}]

    response_text = ""
    completion = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=messages,
        temperature=1,
        max_tokens=1024,
        top_p=1,
        stream=True,
    )

    for chunk in completion:
        if chunk.choices[0].delta.content:
            response_text += chunk.choices[0].delta.content

    suggestions.append(response_text)

# Add suggestions only to the processed rows
df_cleaned_limited = df_cleaned.iloc[:LIMIT].copy()
df_cleaned_limited["suggestion"] = suggestions

# Export to CSV
df_cleaned_limited.to_csv("grades_with_suggestions.csv", index=False)

print("Suggestions added and CSV exported.")

